{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da044aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "import jieba\n",
    "import math\n",
    "import time\n",
    "def getword_f(words):     #这是词频统计函数\n",
    "    word_f_dic = {}\n",
    "    for w in words:\n",
    "        word_f_dic[w] = word_f_dic.get(w, 0) + 1\n",
    "    return word_f_dic.items()\n",
    "if __name__ == '__main__':\n",
    "    before = time.time()\n",
    "    corpus = []\n",
    "    count=0\n",
    "    f=open('./data_new.txt', encoding='utf-8',errors = 'ignore')\n",
    "    f1=open('./cn_stopwords.txt.txt', encoding='utf-8',errors = 'ignore') #将标点符号换成回车符，但可能有多个标点连续存在，产生很多空行\n",
    "    f2=open('./stop_punctuation.txt', encoding='utf-8',errors = 'ignore')#删去无意义的停词\n",
    "    data0=f.read()\n",
    "    data1=f1.read().splitlines()\n",
    "    data2=f2.read().splitlines()\n",
    "    for line in data2:\n",
    "        data0=data0.replace(line,'\\n')\n",
    "    for line in data1:\n",
    "        #print(line)\n",
    "        data0=data0.replace(line,'') \n",
    "    f.close()\n",
    "    f=open('./data_new_new.txt', \"w\",encoding='utf-8',errors = 'ignore')\n",
    "    f.write(data0)\n",
    "    f.close()\n",
    "    f1.close()\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bfb2ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总字数: 4452220\n",
      "分词个数: 4452220\n",
      "平均词长: 1.0\n",
      "基于词的一元模型的中文信息熵为: 8.112 比特/词\n",
      "运行时间: 2.429 s\n"
     ]
    }
   ],
   "source": [
    "#字频统计\n",
    "import jieba\n",
    "import math\n",
    "import time\n",
    "\n",
    "def getword_f(words):     #这是字频统计函数\n",
    "    word_f_dic = {}\n",
    "    for w in words:\n",
    "        word_f_dic[w] = word_f_dic.get(w, 0) + 1\n",
    "    return word_f_dic.items()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    before = time.time()\n",
    "    corpus = []\n",
    "    count=0        \n",
    "    with open('./data_new_new.txt', encoding='utf-8',errors = 'ignore') as f:        \n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                corpus.append(line.strip())\n",
    "                count += len(line.strip())\n",
    "        split_words = []\n",
    "        words_len=0\n",
    "        for line in corpus:\n",
    "            for x in line:\n",
    "                split_words.append(x)\n",
    "                words_len += 1\n",
    "        print(\"总字数:\", count)\n",
    "        word_f_dic = getword_f(split_words)  # 得到字频表\n",
    "        word_list=list(word_f_dic)\n",
    "        word_list=sorted(word_list, key=lambda x:x[1],reverse=True)                #按字频降序排列\n",
    "        with open('./character_f_dic.txt', 'w', encoding='utf-8') as ff:\n",
    "            for k,v in word_list:\n",
    "                ff.write(str(k)+' '+str(v)+'\\n')\n",
    "        ff.close\n",
    "        entropy = [-(uni_word[1]/words_len)*math.log(uni_word[1]/words_len, 2) for uni_word in words_tf]\n",
    "        print(\"基于字的一元模型的中文信息熵为:\", round(sum(entropy), 3), \"比特/词\")\n",
    "    after = time.time()\n",
    "    print(\"运行时间:\", round(after-before, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41388b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料库字数: 4452220\n",
      "分词个数: 2495677\n",
      "平均词长: 1.784\n",
      "基于词的一元模型的中文信息熵为: 13.623 比特/词\n",
      "运行时间: 26.193 s\n"
     ]
    }
   ],
   "source": [
    "#一元词频统计\n",
    "import jieba\n",
    "import math\n",
    "import time\n",
    "\n",
    "def getword_f(words):     #这是字/词频统计函数\n",
    "    word_f_dic = {}\n",
    "    for w in words:\n",
    "        word_f_dic[w] = word_f_dic.get(w, 0) + 1\n",
    "    return word_f_dic.items()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    before = time.time()\n",
    "    corpus = []\n",
    "    count=0        \n",
    "    with open('./data_new_new.txt', encoding='utf-8',errors = 'ignore') as f:        \n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                corpus.append(line.strip())\n",
    "                count += len(line.strip())\n",
    "        split_words = []\n",
    "        words_len=0\n",
    "        for line in corpus:\n",
    "            for x in jieba.cut(line):\n",
    "                split_words.append(x)\n",
    "                words_len += 1\n",
    "        print(\"总字数:\", count)\n",
    "        print(\"词数:\", words_len)\n",
    "        print(\"平均词长:\", round(count/words_len, 3))\n",
    "        word_f_dic = getword_f(split_words)  # 得到词频表\n",
    "        word_list=list(word_f_dic)\n",
    "        word_list=sorted(word_list, key=lambda x:x[1],reverse=True)                #按词频降序排列\n",
    "        with open('./word_f_dic.txt', 'w', encoding='utf-8') as ff:\n",
    "            for k,v in word_list:\n",
    "                ff.write(str(k)+' '+str(v)+'\\n')\n",
    "        ff.close\n",
    "        Information_entropy = [-(uni_word[1]/words_len)*math.log(uni_word[1]/words_len, 2) for uni_word in words_tf]\n",
    "        print(\"基于词的一元模型的中文信息熵为:\", round(sum(Information_entropy), 3), \"比特/词\")\n",
    "    after = time.time()\n",
    "    print(\"运行时间:\", round(after-before, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d67cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料库字数: 4452220\n",
      "分词个数: 2495677\n",
      "平均词长: 1.784\n",
      "语料行数: 1030181\n",
      "二元模型长度: 1470843\n",
      "基于词的二元模型的中文信息熵为: 5.599 比特/词\n",
      "运行时间: 31.391 s\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import math\n",
    "import time\n",
    "def getword_f(tf_dic, words):\n",
    "    for i in range(len(words)-1):\n",
    "        tf_dic[words[i]] = tf_dic.get(words[i], 0) + 1\n",
    "def get_biword_f(tf_dic, words):\n",
    "    for i in range(len(words)-1):\n",
    "        tf_dic[(words[i], words[i+1])] = tf_dic.get((words[i], words[i+1]), 0) + 1\n",
    "if __name__ == '__main__':\n",
    "    before = time.time()\n",
    "    with open('./data_new_new.txt', encoding='utf-8',errors = 'ignore') as f :\n",
    "        corpus = []\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                corpus.append(line.strip())\n",
    "                count += len(line.strip())\n",
    "    split_words = []\n",
    "    words_len = 0\n",
    "    line_count = 0\n",
    "    words_tf = {}\n",
    "    bigram_tf = {}\n",
    "    for line in corpus:\n",
    "        for x in jieba.cut(line):\n",
    "            split_words.append(x)\n",
    "            words_len += 1\n",
    "        getword_f(words_tf, split_words)\n",
    "        get_biword_f(bigram_tf, split_words)\n",
    "        split_words = []\n",
    "        line_count += 1\n",
    "    print(\"语料库字数:\", count)\n",
    "    print(\"分词个数:\", words_len)\n",
    "    print(\"平均词长:\", round(count / words_len, 3))\n",
    "    print(\"语料行数:\", line_count)\n",
    "    bigram_len = sum([dic[1] for dic in bigram_tf.items()])\n",
    "    print(\"二元模型长度:\", bigram_len)\n",
    "    entropy = []\n",
    "    for bi_word in bigram_tf.items():\n",
    "        jp_xy = bi_word[1] / bigram_len  # 计算联合概率p(x,y)\n",
    "        cp_xy = bi_word[1] / words_tf[bi_word[0][0]]  # 计算条件概率p(x|y)\n",
    "        entropy.append(-jp_xy * math.log(cp_xy, 2))  # 计算二元模型的信息熵\n",
    "    print(\"基于词的二元模型的中文信息熵为:\", round(sum(entropy), 3), \"比特/词\")  # 6.402\n",
    "    after = time.time()\n",
    "    print(\"运行时间:\", round(after-before, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a52c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sorted_list(d, reverse=True):\n",
    "    return sorted(d.items(), key=lambda x:x[1], reverse=reverse)\n",
    "biagram_list=get_sorted_list(bigram_tf)\n",
    "with open(\"2cipin.txt\", \"w\",encoding='utf-8',errors = 'ignore') as file:\n",
    "    for item1,item2 in biagram_list:\n",
    "        file.write(str(item1)+' '+str(item2)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd28954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_list(d, reverse=True):\n",
    "    return sorted(d.items(), key=lambda x:x[1], reverse=reverse)\n",
    "words_list=get_sorted_list(words_tf)\n",
    "with open(\"2feimocipin.txt\", \"w\",encoding='utf-8',errors = 'ignore') as file:\n",
    "    for item1,item2 in words_list:\n",
    "        file.write(str(item1)+' '+str(item2)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18623a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料库字数: 4452220\n",
      "分词个数: 2495677\n",
      "平均词长: 1.784\n",
      "语料行数: 1030181\n",
      "三元模型长度: 714929\n",
      "基于词的三元模型的中文信息熵为: 0.875 比特/词\n",
      "运行时间: 32.553 s\n"
     ]
    }
   ],
   "source": [
    "# %load trigram_entropy.py\n",
    "import jieba\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "# 非句子末尾二元词频统计\n",
    "def get_biword_f(tf_dic, words):\n",
    "    for i in range(len(words)-2):\n",
    "        tf_dic[(words[i], words[i+1])] = tf_dic.get((words[i], words[i+1]), 0) + 1\n",
    "\n",
    "\n",
    "# 三元模型词频统计\n",
    "def get_triword_f(tf_dic, words):\n",
    "    for i in range(len(words)-2):\n",
    "        tf_dic[((words[i], words[i+1]), words[i+2])] = tf_dic.get(((words[i], words[i+1]), words[i+2]), 0) + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    before = time.time()\n",
    "\n",
    "    with open('./data_new_new.txt', encoding='utf-8',errors = 'ignore') as f:\n",
    "        corpus = []\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                corpus.append(line.strip())\n",
    "                count += len(line.strip())\n",
    "    split_words = []\n",
    "    words_len = 0\n",
    "    line_count = 0\n",
    "    words_tf = {}\n",
    "    trigram_tf = {}\n",
    "\n",
    "    for line in corpus:\n",
    "        for x in jieba.cut(line):\n",
    "            split_words.append(x)\n",
    "            words_len += 1\n",
    "\n",
    "        get_biword_f(words_tf, split_words)\n",
    "        get_triword_f(trigram_tf, split_words)\n",
    "\n",
    "        split_words = []\n",
    "        line_count += 1\n",
    "    print(\"语料库字数:\", count)\n",
    "    print(\"分词个数:\", words_len)\n",
    "    print(\"平均词长:\", round(count / words_len, 3))\n",
    "    print(\"语料行数:\", line_count)\n",
    "    with open(\"3cipin.txt\", \"w\",encoding='utf-8',errors = 'ignore') as file:\n",
    "        for item in list(bigram_tf):\n",
    "            file.write(str(item) + \"\\n\")\n",
    "    trigram_len = sum([dic[1] for dic in trigram_tf.items()])\n",
    "    print(\"三元模型长度:\", trigram_len)\n",
    "    entropy = []\n",
    "    for tri_word in trigram_tf.items():\n",
    "        jp_xy = tri_word[1] / trigram_len  # 计算联合概率p(x,y)\n",
    "        cp_xy = tri_word[1] / words_tf[tri_word[0][0]]  # 计算条件概率p(x|y)\n",
    "        entropy.append(-jp_xy * math.log(cp_xy, 2))  # 计算三元模型的信息熵\n",
    "    print(\"基于词的三元模型的中文信息熵为:\", round(sum(entropy), 3), \"比特/词\")  # 0.936\n",
    "\n",
    "    after = time.time()\n",
    "    print(\"运行时间:\", round(after-before, 3), \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b05fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_list(d, reverse=True):\n",
    "    return sorted(d.items(), key=lambda x:x[1], reverse=reverse)\n",
    "words_list=get_sorted_list(words_tf)\n",
    "with open(\"3feimocipin.txt\", \"w\",encoding='utf-8',errors = 'ignore') as file:\n",
    "    for item1,item2 in words_list:\n",
    "        file.write(str(item1)+' '+str(item2)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f3aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_list(d, reverse=True):\n",
    "    return sorted(d.items(), key=lambda x:x[1], reverse=reverse)\n",
    "trigram_list=get_sorted_list(trigram_tf)\n",
    "with open(\"3cipin.txt\", \"w\",encoding='utf-8',errors = 'ignore') as file:\n",
    "    for item1,item2 in trigram_list:\n",
    "        file.write(str(item1)+' '+str(item2)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
